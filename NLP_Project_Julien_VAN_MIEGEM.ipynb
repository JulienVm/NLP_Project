{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project_Julien_VAN_MIEGEM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1L_tDGQN58F9-ud1sZhFsFh-y3d7vgrS3",
      "authorship_tag": "ABX9TyOZStn3H0C/DhGy8W3S17Cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulienVm/NLP_Project/blob/main/NLP_Project_Julien_VAN_MIEGEM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-K3GVKlxzzG"
      },
      "source": [
        "# Projet NLP - Analyse de sentiments\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK9laGAwYx1l"
      },
      "source": [
        "## Importations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pndydV3NvCa-",
        "outputId": "d22d0cd1-7425-4e0e-c843-0e6133083c81"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBQGJ6UH2d9X",
        "outputId": "7ecd7573-0869-42a4-ab8b-32b483d172ff"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import nltk\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pickle\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC91e-Oz24jx"
      },
      "source": [
        "## Création du set de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zFOrZFEh1djQ",
        "outputId": "6d9af20d-4974-4abd-c291-156ddd275c72"
      },
      "source": [
        "nRowsRead = 10000 # le nombre de lignes qu'on lit dans notre csv (cela prendrait trop de temps de tout lire)\r\n",
        "offset=0 # pour décaler les indexes de début et de fin de nos lignes lues dans le csv\r\n",
        "nRowsSkipped = [i for i in range(offset+nRowsRead-nRowsRead//2,1599998-nRowsRead//2-offset)] # liste des indexes des lignes non lues dans notre csv\r\n",
        "data=pd.read_csv(\"drive/MyDrive/NLP/data.csv\", encoding=\"ISO-8859-1\", skiprows=nRowsSkipped, nrows=nRowsRead, header=None, names=['target','id','date','flag','user','text'], dtype={'target':np.int32,'id':np.int32,'date':np.string_,'flag':np.string_,'user':np.string_,\"text\":np.string_})\r\n",
        "data.head() # on a un dataframe à 6 colonnes : le sentiment entre 0 pour négatif, 2 pour neutre et 4 pour positif, l'id, la date, le flag, l'utilisateur et le texte"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                               text\n",
              "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1       0  ...  is upset that he can't update his Facebook by ...\n",
              "2       0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3       0  ...    my whole body feels itchy and like its on fire \n",
              "4       0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Ja-avOvLZc"
      },
      "source": [
        "##Extraction des données utiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "cmht2gCxvprq",
        "outputId": "f94c5b8d-2068-41de-9daa-e52c7f48771b"
      },
      "source": [
        "# selon la consigne de l'exercice, on doit retirer les tweets neutres labélisés à 2\r\n",
        "indexToDelete=[] # liste des indexes des lignes à enlever dans notre dataframe\r\n",
        "for i in range(len(data['target'])):\r\n",
        "  if data['target'][i]==2:\r\n",
        "    indexToDelete.append(i)\r\n",
        "print(\"Removed\",len(indexToDelete),\"rows.\")\r\n",
        "data.drop(indexToDelete) # drop permet d'enlever les lignes correspondant aux indexes listés"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed 0 rows.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>4</td>\n",
              "      <td>-2101387819</td>\n",
              "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ChloeAmisha</td>\n",
              "      <td>@SCOOBY_GRITBOYS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>4</td>\n",
              "      <td>-2101387807</td>\n",
              "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>EvolveTom</td>\n",
              "      <td>@Cliff_Forster Yeah, that does work better tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>4</td>\n",
              "      <td>-2101365330</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>4</td>\n",
              "      <td>-2101365327</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>4</td>\n",
              "      <td>-2101365305</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      target  ...                                               text\n",
              "0          0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  ...  is upset that he can't update his Facebook by ...\n",
              "2          0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3          0  ...    my whole body feels itchy and like its on fire \n",
              "4          0  ...  @nationwideclass no, it's not behaving at all....\n",
              "...      ...  ...                                                ...\n",
              "9995       4  ...                                  @SCOOBY_GRITBOYS \n",
              "9996       4  ...  @Cliff_Forster Yeah, that does work better tha...\n",
              "9997       4  ...  Just woke up. Having no school is the best fee...\n",
              "9998       4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
              "9999       4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
              "\n",
              "[10000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfvu1q_bnkk5"
      },
      "source": [
        "##Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7hSKqEpn6Yp",
        "outputId": "09aafcba-d551-4d05-bd27-3a45fffa6477"
      },
      "source": [
        "feelingList=data['target']/4 # on normalise les labels de tweets positifs à 1 au lieu de 4, et on met le tout dans une liste\r\n",
        "tweetList=data['text'] # liste contenant les tweets\r\n",
        "tweetList[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1    is upset that he can't update his Facebook by ...\n",
              "2    @Kenichan I dived many times for the ball. Man...\n",
              "3      my whole body feels itchy and like its on fire \n",
              "4    @nationwideclass no, it's not behaving at all....\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRF9cNol9CVv"
      },
      "source": [
        "## Mise en forme des données\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErM8KtDBKZbC"
      },
      "source": [
        "###Formation de vecteurs de mots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnPhT_BA_6pW",
        "outputId": "6e5d6693-1302-4a51-bf72-7d571d6680fe"
      },
      "source": [
        "from nltk.stem.porter import *\r\n",
        "stemmer=stemmer = PorterStemmer() # on prend le porter stemmer car c'est en anglais\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "for i in range(len(tweetList)):\r\n",
        "  tweet=\"\"\r\n",
        "  for char in tweetList[i]:\r\n",
        "    if char not in [',','!','(',')','?',':','[',']',';','-','+']: # on remplace la ponctuation par des points\r\n",
        "      tweet+=char\r\n",
        "    else:\r\n",
        "      tweet+='.'\r\n",
        "  tweet=tweet.lower() # on met en minuscule\r\n",
        "  tweet=word_tokenize(tweet) # tokenisation\r\n",
        "  tweet=[w for w in tweet if not w in list(nltk.corpus.stopwords.words('english'))] # on enlève les stopwords\r\n",
        "  tweet=[stemmer.stem(w) for w in tweet] #stemming = on enlève la conjugaison, les pluriels ...\r\n",
        "  tweet=[Word_Lemmatizer.lemmatize(w) for w in tweet] # lemmatisation = on ne garde que la racine des mots\r\n",
        "  tweet=[w for w in tweet if ('@' not in w) and ('http' not in w)] # on enlève certains mots jugés inutiles (mentions, liens)\r\n",
        "  tweetList[i]=tweet\r\n",
        "tweetList[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [switchfoot, ., awww, ., 's, bummer, ., should...\n",
              "1    [upset, ca, n't, updat, facebook, text, ..., m...\n",
              "2    [kenichan, dive, mani, time, ball, ., manag, s...\n",
              "3               [whole, bodi, feel, itchi, like, fire]\n",
              "4    [nationwideclass, ., 's, behav, ., 'm, mad, .,...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibkkG7YtKMeL"
      },
      "source": [
        "###Création du set de vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkQjk7bRHN7S",
        "outputId": "69024e83-ad6e-4dcb-a69b-467d8fcd93f1"
      },
      "source": [
        "V=set() # notre vocabulaire, un ensemble contenant chaque mot rencontré dans les tweets étudiés\r\n",
        "for tweet in tweetList :\r\n",
        "  for word in tweet :\r\n",
        "    V.add(word)\r\n",
        "len(V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo_Q-rLKNwdt"
      },
      "source": [
        "###Création de bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "LCmKd9lPLEFl",
        "outputId": "b5ddff47-2a98-46a6-9b12-62c37f8ae2ac"
      },
      "source": [
        "list_bags=[] # liste qui contiendra les bags of words\r\n",
        "for tweet in tweetList:\r\n",
        "  bag=np.zeros(len(V)) # vecteur de 0 de dimension égale à la taille du vocabulaire\r\n",
        "  i=0\r\n",
        "  for word in V:\r\n",
        "    if word in tweet :\r\n",
        "      bag[i]=tweet.count(word) # à chaque fois qu'un mot du vocablaire est dans un tweet, on met à la coordonnée du vecteur correspondant à ce mot le nombre d'occurences du mot dans le tweet\r\n",
        "    i+=1\r\n",
        "  list_bags.append(bag)\r\n",
        "\r\n",
        "list_bags=np.array(list_bags)\r\n",
        "finalData=pd.DataFrame(list_bags,columns=V) # dataframe pour visualiser nos données finales\r\n",
        "finalData.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gtalk</th>\n",
              "      <th>becoz</th>\n",
              "      <th>otah</th>\n",
              "      <th>j</th>\n",
              "      <th>andrerib</th>\n",
              "      <th>spiral</th>\n",
              "      <th>myki</th>\n",
              "      <th>greed</th>\n",
              "      <th>quot.hous</th>\n",
              "      <th>elbow</th>\n",
              "      <th>soundcheck</th>\n",
              "      <th>rori</th>\n",
              "      <th>homestead</th>\n",
              "      <th>file</th>\n",
              "      <th>camilla</th>\n",
              "      <th>masih</th>\n",
              "      <th>class..</th>\n",
              "      <th>robertzalm</th>\n",
              "      <th>whoschrishugh</th>\n",
              "      <th>today.</th>\n",
              "      <th>tme</th>\n",
              "      <th>despr</th>\n",
              "      <th>boymonst</th>\n",
              "      <th>jacob</th>\n",
              "      <th>hold</th>\n",
              "      <th>writer/poet</th>\n",
              "      <th>quot.drag</th>\n",
              "      <th>gruel</th>\n",
              "      <th>wanted2</th>\n",
              "      <th>martin</th>\n",
              "      <th>auto.small.cap</th>\n",
              "      <th>button..goodnight</th>\n",
              "      <th>movingtomontana</th>\n",
              "      <th>jeremi</th>\n",
              "      <th>josordoni</th>\n",
              "      <th>rachael90210</th>\n",
              "      <th>aberdeen</th>\n",
              "      <th>var</th>\n",
              "      <th>catchbifucan</th>\n",
              "      <th>citrusandcandi</th>\n",
              "      <th>...</th>\n",
              "      <th>syifa</th>\n",
              "      <th>faerywitch</th>\n",
              "      <th>stole</th>\n",
              "      <th>workng</th>\n",
              "      <th>179</th>\n",
              "      <th>wz</th>\n",
              "      <th>ttsc</th>\n",
              "      <th>august</th>\n",
              "      <th>librari</th>\n",
              "      <th>die</th>\n",
              "      <th>v.upset</th>\n",
              "      <th>shiti</th>\n",
              "      <th>brng</th>\n",
              "      <th>hou</th>\n",
              "      <th>megmcc</th>\n",
              "      <th>missytigg</th>\n",
              "      <th>candychao</th>\n",
              "      <th>senior</th>\n",
              "      <th>nerdi</th>\n",
              "      <th>dang</th>\n",
              "      <th>slacker</th>\n",
              "      <th>brought</th>\n",
              "      <th>'goocharama</th>\n",
              "      <th>week..</th>\n",
              "      <th>inesvarga</th>\n",
              "      <th>.aka</th>\n",
              "      <th>puberty..</th>\n",
              "      <th>ceiling.</th>\n",
              "      <th>ill.</th>\n",
              "      <th>carpet</th>\n",
              "      <th>saffc</th>\n",
              "      <th>muscl</th>\n",
              "      <th>pad</th>\n",
              "      <th>pissed/hungri</th>\n",
              "      <th>expeci</th>\n",
              "      <th>gulf</th>\n",
              "      <th>marcofrissen</th>\n",
              "      <th>quot.which</th>\n",
              "      <th>kazzc22</th>\n",
              "      <th>musik</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 17182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   gtalk  becoz  otah    j  ...  marcofrissen  quot.which  kazzc22  musik\n",
              "0    0.0    0.0   0.0  0.0  ...           0.0         0.0      0.0    0.0\n",
              "1    0.0    0.0   0.0  0.0  ...           0.0         0.0      0.0    0.0\n",
              "2    0.0    0.0   0.0  0.0  ...           0.0         0.0      0.0    0.0\n",
              "3    0.0    0.0   0.0  0.0  ...           0.0         0.0      0.0    0.0\n",
              "4    0.0    0.0   0.0  0.0  ...           0.0         0.0      0.0    0.0\n",
              "\n",
              "[5 rows x 17182 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8T_jQsRol2r"
      },
      "source": [
        "## Sauvegarde du prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlyiBlnCoxAV"
      },
      "source": [
        "pickle_out = open(\"drive/MyDrive/NLP/list_bags.pickle\",\"wb\") # on sauvegarde list_bags (les bag of words) pour ne pas avoir à refaire le prétraitement lorsque l'on réexécute le notebook\r\n",
        "pickle.dump(list_bags, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open(\"drive/MyDrive/NLP/feelingList.pickle\",\"wb\") # idem pour feelingList (liste des labels)\r\n",
        "pickle.dump(feelingList, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open(\"drive/MyDrive/NLP/V.pickle\",\"wb\") # idem pour V (le vocabulaire)\r\n",
        "pickle.dump(V, pickle_out)\r\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Ui3S41RgrE"
      },
      "source": [
        "##Création du modèle simple de référence\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcM2CwUTp_HJ"
      },
      "source": [
        "# on importe les données sauvegardées\r\n",
        "# Lorsqu'on réexécute le notebook, il suffit d'exécuter la cellule au début pour les importations, puis on passe directement à celle-ci\r\n",
        "pickle_in=open(\"drive/MyDrive/NLP/list_bags.pickle\",\"rb\")\r\n",
        "x=pickle.load(pickle_in)\r\n",
        "pickle_in.close()\r\n",
        "\r\n",
        "pickle_in=open(\"drive/MyDrive/NLP/feelingList.pickle\",\"rb\")\r\n",
        "y=pickle.load(pickle_in)\r\n",
        "pickle_in.close()\r\n",
        "\r\n",
        "pickle_in=open(\"drive/MyDrive/NLP/V.pickle\",\"rb\")\r\n",
        "V=pickle.load(pickle_in)\r\n",
        "pickle_in.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN18EY_6RoPB",
        "outputId": "f9af2626-a7eb-4f83-80d6-3df9c7482830"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn import metrics\r\n",
        "import seaborn as sn\r\n",
        "from sklearn import linear_model\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8,random_state=0) # on sépare nos données en un jeu de test et un jeu d'entraînement\r\n",
        "\r\n",
        "modele_regLog = linear_model.LogisticRegression(random_state = 0,solver = 'liblinear', multi_class = 'auto') # initialisation d'un modèle simple de régression logistique\r\n",
        "\r\n",
        "modele_regLog.fit(x_train,y_train) # entraînement du modèle\r\n",
        "\r\n",
        "precision = modele_regLog.score(x_test,y_test) # test du modèle\r\n",
        "print(\"Nous avons une précision de\",precision*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nous avons une précision de 75.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZRsjBQ_Y83r"
      },
      "source": [
        "##Un premier RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idrHJ6tmOSGG"
      },
      "source": [
        "### Création d'un premier RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBOP9UwbM_vy"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.models import load_model\r\n",
        "from keras.layers.core import Dense\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Flatten\r\n",
        "from os.path import exists\r\n",
        "\r\n",
        "if exists(\"drive/MyDrive/NLP/model1.h5\"):\r\n",
        "  model1=load_model(\"drive/MyDrive/NLP/model1.h5\",compile=False) # on charge le modèle s'il existe déjà\r\n",
        "else :\r\n",
        "  model1=Sequential() # sinon, on en itialise un nouveau\r\n",
        "  model1.add(Dense(64, activation='relu')) # c'est un premier modèle \"naïf\" qui n'utilise qu'une couche dense\r\n",
        "  model1.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBODDEkedUJ5"
      },
      "source": [
        "### Entraînement d'un premier RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtDOc9TOwOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf156393-ae40-44db-e9dc-687c3396d555"
      },
      "source": [
        "model1.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model1.fit(x_train, y_train,epochs=5, batch_size=10, verbose=1) # entraînement du modèle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5092\n",
            "Epoch 2/5\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5062\n",
            "Epoch 3/5\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5056\n",
            "Epoch 4/5\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5144\n",
            "Epoch 5/5\n",
            "800/800 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 0.4954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f63b620f990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14cBMDXoZO4d"
      },
      "source": [
        "###Sauvegarde"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ATMxKbZUOh"
      },
      "source": [
        "model1.save(\"drive/MyDrive/NLP/model1.h5\",overwrite=True) # on sauvegarde le drive dans le drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oloh5zXXsdh4"
      },
      "source": [
        "###Test du RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mL-nQOXi6QS",
        "outputId": "e2d22687-3bbc-44f0-d9d0-fcf5fba4d6bd"
      },
      "source": [
        "model1.evaluate(x_test,y_test) # on évalue le modèle sur les données de test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.5683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 1.568253993988037]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QGedVpXdZup"
      },
      "source": [
        "def pretraitement(initTweet): # fonction qui reprend la méthode de prétraitement qu'on a vu plus haut : elle prend en entrée une phrase et on retourne le bag of words\r\n",
        "  tweet=\"\"\r\n",
        "  for char in initTweet:\r\n",
        "    if char not in [',','!','(',')','?',':','[',']',';','-','+']: # on remplace la ponctuation par des points\r\n",
        "      tweet+=char\r\n",
        "    else:\r\n",
        "      tweet+='.'\r\n",
        "  tweet=tweet.lower() # on met en minuscule\r\n",
        "  tweet=word_tokenize(tweet) # tokenisation\r\n",
        "  tweet=[w for w in tweet if not w in list(nltk.corpus.stopwords.words('english'))] # on enlève les stopwords\r\n",
        "  tweet=[stemmer.stem(w) for w in tweet] #stemming = on enlève la conjugaison, les pluriels ...\r\n",
        "  tweet=[Word_Lemmatizer.lemmatize(w) for w in tweet] # lemmatisation = on ne garde que la racine des mots\r\n",
        "  tweet=[w for w in tweet if ('@' not in w) and ('http' not in w)] # on enlève certains mots jugés inutiles (mentions, liens...)\r\n",
        "  bag=np.zeros(len(V))\r\n",
        "  i=0\r\n",
        "  for word in V:\r\n",
        "    if word in tweet :\r\n",
        "      bag[i]=tweet.count(word)\r\n",
        "    i+=1\r\n",
        "  return bag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BX-GXzGfp_3",
        "outputId": "354cba31-ab88-4fa7-97f0-a0b2663cb20a"
      },
      "source": [
        "test=\"I hate\" # testons notre pipeline avec une phrase\r\n",
        "test=pretraitement(test)\r\n",
        "print(model1.predict(np.array([test]))[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdnrObcBlF24"
      },
      "source": [
        "##On teste avec l'API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUrbdK7QlJDu",
        "outputId": "d33f726a-3fa7-4646-d97c-dc97444293dc"
      },
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "api_key='guzYPRiLoRC1YWkpo0UMfFofIR9WykNMEZmtCBhazcCyvkohbkirOlslWZ-siZr-3pXr7XDcdoxSpczkyEp0uMJr_148T3kHKmjTd99saCVON3e-_qRhEC3Q8EI6YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\" # n'ayant pas trouvé le json de tweets, je prends des commentaires de restaurants\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "print('the status code is {}'.format(req.status_code))\r\n",
        "\r\n",
        "\r\n",
        "dataAPI=json.loads(req.text)\r\n",
        "dataAPI"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the status code is 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'possible_languages': ['fr', 'en', 'zh', 'pt', 'de', 'it', 'sv', 'ja', 'es'],\n",
              " 'reviews': [{'id': 'qOiuCTz_Gzgmose1GHaAlg',\n",
              "   'rating': 5,\n",
              "   'text': \"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\",\n",
              "   'time_created': '2020-12-19 07:52:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=j_T35sgsu3IpWm0V3qF5Zg&hrid=qOiuCTz_Gzgmose1GHaAlg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=j_T35sgsu3IpWm0V3qF5Zg',\n",
              "   'user': {'id': 'FRKnLBnlFiasr1Dc8oOIGQ',\n",
              "    'image_url': 'https://s3-media3.fl.yelpcdn.com/photo/1wm0yjWSw92j_ZsUFZBGzQ/o.jpg',\n",
              "    'name': 'Sarah G.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=FRKnLBnlFiasr1Dc8oOIGQ'}},\n",
              "  {'id': '7d56A_ObMPHHyywcfhnrUw',\n",
              "   'rating': 5,\n",
              "   'text': 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...',\n",
              "   'time_created': '2021-02-25 19:56:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=j_T35sgsu3IpWm0V3qF5Zg&hrid=7d56A_ObMPHHyywcfhnrUw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=j_T35sgsu3IpWm0V3qF5Zg',\n",
              "   'user': {'id': '77MIRZBZqbQKoonun8qWjQ',\n",
              "    'image_url': 'https://s3-media2.fl.yelpcdn.com/photo/ptL063kjqRUDQ6pny96iqg/o.jpg',\n",
              "    'name': 'Ipsita D.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=77MIRZBZqbQKoonun8qWjQ'}},\n",
              "  {'id': 'SYykUNBT2dJznMV5PpRnhg',\n",
              "   'rating': 5,\n",
              "   'text': 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...',\n",
              "   'time_created': '2020-12-12 15:19:49',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=j_T35sgsu3IpWm0V3qF5Zg&hrid=SYykUNBT2dJznMV5PpRnhg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=j_T35sgsu3IpWm0V3qF5Zg',\n",
              "   'user': {'id': 'yw2cJk_SfGZlcoZKEUevxw',\n",
              "    'image_url': 'https://s3-media1.fl.yelpcdn.com/photo/EH2WTffzgKs72GHqa4kn6A/o.jpg',\n",
              "    'name': 'Thomas V.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=yw2cJk_SfGZlcoZKEUevxw'}}],\n",
              " 'total': 5609}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PyeweylssfA",
        "outputId": "7c964711-9e30-4f0b-dce8-874a421e2239"
      },
      "source": [
        "nTests=3\r\n",
        "\r\n",
        "for i in range(nTests):\r\n",
        "  testData =dataAPI[\"reviews\"][i][\"text\"]\r\n",
        "  print(testData)\r\n",
        "  testData=pretraitement(testData)\r\n",
        "  prediction=model1.predict(np.array([testData]))[0][0] # prediction de notre modèle entre 0 et 1\r\n",
        "  if (prediction<0.4):\r\n",
        "    print(\"This comment is negative.\")\r\n",
        "  elif (prediction>0.6):\r\n",
        "    print(\"This comment is postive.\")\r\n",
        "  else:\r\n",
        "    print(\"This comment is neutral.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\n",
            "This comment is negative.\n",
            "Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...\n",
            "This comment is negative.\n",
            "Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...\n",
            "This comment is negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKDsFmom6-KT"
      },
      "source": [
        "##Second RNN plus évolué"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZXmP8PV8_X5"
      },
      "source": [
        "###Nouvelle forme de prétraitement avec Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqRBc5e_9EAi",
        "outputId": "3eef1e22-5af8-4636-cfd0-110e484c6e60"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "nRowsRead = 20000 # le nombre de lignes qu'on lit dans notre csv\r\n",
        "offset=0 # pour décaler les indexes de début et de fin de nos lignes lues dans le csv\r\n",
        "nRowsSkipped = [i for i in range(offset+nRowsRead-nRowsRead//2,1599998-nRowsRead//2-offset)] # liste des indexes des lignes non lues dans notre csv\r\n",
        "data2=pd.read_csv(\"drive/MyDrive/NLP/data.csv\", encoding=\"ISO-8859-1\", skiprows=nRowsSkipped, nrows=nRowsRead, header=None, names=['target','id','date','flag','user','text'], dtype={'target':np.int32,'id':np.int32,'date':np.string_,'flag':np.string_,'user':np.string_,\"text\":np.string_})\r\n",
        "\r\n",
        "for i in range(len(data2[\"text\"])):\r\n",
        "    data2[\"text\"][i]=data2[\"text\"][i].lower()\r\n",
        "\r\n",
        "nWords=5000 # nombre de mots de vocabulaire qu'on garde : on ne garde que les plus\r\n",
        "tokenizer = Tokenizer(num_words=nWords, split=\" \") # faisons un essai avec le prétraitement de keras : il sera peut-être plus adapté pour le RNN\r\n",
        "tokenizer.fit_on_texts(data2[\"text\"].values)\r\n",
        "\r\n",
        "x2 = tokenizer.texts_to_sequences(data2[\"text\"].values)\r\n",
        "x2 = pad_sequences(x2)\r\n",
        "\r\n",
        "y2=pd.get_dummies(data2[\"target\"]).values # on met 0 1 pour 0 et 1 0 pour 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDkkYpTI7T6v"
      },
      "source": [
        "###Création"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nljZFQTd7BVg",
        "outputId": "3d33f1b5-95cc-45f4-a224-3fa92b599bda"
      },
      "source": [
        "if exists(\"drive/MyDrive/NLP/model2.h5\"):\r\n",
        "  model2=load_model(\"drive/MyDrive/NLP/model2.h5\",compile=False) # on charge le modèle s'il existe déjà\r\n",
        "else :\r\n",
        "  model2=Sequential() # sinon, on en itialise un nouveau\r\n",
        "  model2.add(Embedding(nWords,128,input_length=x2.shape[1])) # l'Embedding layer nous permet de convertir chaque mot en un vecteur de longueur fixée, qui a des valeurs autres que 0 et 1\r\n",
        "  model2.add(Dropout(0.3)) # pour éviter l'overfitting\r\n",
        "  model2.add(LSTM(128,return_sequences=False,dropout=0.3,recurrent_dropout=0.2)) # le lstm permet de repérer l'information utile\r\n",
        "  model2.add(Dense(32, activation='relu'))\r\n",
        "  model2.add(Dense(2, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AblFs_e57WNQ"
      },
      "source": [
        "###Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTvS4GAojER1"
      },
      "source": [
        "x2_train,x2_test,y2_train,y2_test = train_test_split(x2,y2,train_size=0.8,random_state=0) # on sépare nos données en un jeu de test et un jeu d'entraînement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvvgthJp7YFe",
        "outputId": "81558963-e43e-4265-f8da-15c6d9da68da"
      },
      "source": [
        "model2.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model2.fit(x2_train, y2_train,epochs=15, batch_size=20, verbose=1) # entraînement du modèle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "800/800 [==============================] - 51s 62ms/step - loss: 0.0669 - accuracy: 0.9724\n",
            "Epoch 2/15\n",
            "800/800 [==============================] - 49s 61ms/step - loss: 0.0617 - accuracy: 0.9757\n",
            "Epoch 3/15\n",
            "800/800 [==============================] - 48s 60ms/step - loss: 0.0560 - accuracy: 0.9790\n",
            "Epoch 4/15\n",
            "800/800 [==============================] - 49s 62ms/step - loss: 0.0515 - accuracy: 0.9788\n",
            "Epoch 5/15\n",
            "800/800 [==============================] - 49s 61ms/step - loss: 0.0474 - accuracy: 0.9811\n",
            "Epoch 6/15\n",
            "800/800 [==============================] - 48s 60ms/step - loss: 0.0449 - accuracy: 0.9816\n",
            "Epoch 7/15\n",
            "800/800 [==============================] - 50s 62ms/step - loss: 0.0432 - accuracy: 0.9821\n",
            "Epoch 8/15\n",
            "800/800 [==============================] - 49s 62ms/step - loss: 0.0408 - accuracy: 0.9827\n",
            "Epoch 9/15\n",
            "800/800 [==============================] - 51s 64ms/step - loss: 0.0367 - accuracy: 0.9859\n",
            "Epoch 10/15\n",
            "800/800 [==============================] - 49s 61ms/step - loss: 0.0298 - accuracy: 0.9878\n",
            "Epoch 11/15\n",
            "800/800 [==============================] - 49s 61ms/step - loss: 0.0317 - accuracy: 0.9887\n",
            "Epoch 12/15\n",
            "800/800 [==============================] - 50s 62ms/step - loss: 0.0296 - accuracy: 0.9877\n",
            "Epoch 13/15\n",
            "800/800 [==============================] - 50s 62ms/step - loss: 0.0307 - accuracy: 0.9879\n",
            "Epoch 14/15\n",
            "800/800 [==============================] - 48s 60ms/step - loss: 0.0363 - accuracy: 0.9867\n",
            "Epoch 15/15\n",
            "800/800 [==============================] - 48s 60ms/step - loss: 0.0258 - accuracy: 0.9894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f64644189d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqLOpFyB7Z0s"
      },
      "source": [
        "###Sauvegarde"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDMkRmRR7bNv"
      },
      "source": [
        "model2.save(\"drive/MyDrive/NLP/model2.h5\",overwrite=True)  # on évalue le modèle sur les données de test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTvX5gPb7gKN"
      },
      "source": [
        "###Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXpK7Cpi7hoP",
        "outputId": "ef0a3ffa-d5aa-423e-d2cc-a54a13553531"
      },
      "source": [
        "model2.evaluate(x2_test,y2_test) # on évalue le modèle sur les données de test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 2s 12ms/step - loss: 1.9131 - accuracy: 0.7275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.91311776638031, 0.7275000214576721]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHC1FfscQd0d",
        "outputId": "fd1e06ec-2c7f-4d03-d7a5-124ce3aa1d00"
      },
      "source": [
        "nTests=3\r\n",
        "\r\n",
        "for i in range(nTests):\r\n",
        "  testData =dataAPI[\"reviews\"][i][\"text\"]\r\n",
        "  print(testData)\r\n",
        "  testData=pretraitement(testData)\r\n",
        "  prediction=model2.predict(np.array([testData]))[0][0] # prediction de notre modèle entre 0 et 1\r\n",
        "  if (prediction<0.4):\r\n",
        "    print(\"This comment is negative.\")\r\n",
        "  elif (prediction>0.6):\r\n",
        "    print(\"This comment is postive.\")\r\n",
        "  else:\r\n",
        "    print(\"This comment is neutral.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 38) for input KerasTensor(type_spec=TensorSpec(shape=(None, 38), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 17182).\n",
            "0.4752633\n",
            "This comment is neutral.\n",
            "Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...\n",
            "0.4752633\n",
            "This comment is neutral.\n",
            "Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...\n",
            "0.4752633\n",
            "This comment is neutral.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuJdJhP1UbpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a622c2bf-2add-40a3-b71d-f543ac90f08b"
      },
      "source": [
        "prediction=model2.predict(np.array([pretraitement(\"I hate this\")]))[0][0] # testons notre pipeline avec une phrase\r\n",
        "print(prediction)\r\n",
        "if (prediction<0.4):\r\n",
        "  print(\"This comment is negative.\")\r\n",
        "elif (prediction>0.6):\r\n",
        "  print(\"This comment is postive.\")\r\n",
        "else:\r\n",
        "  print(\"This comment is neutral.\")\r\n",
        "\r\n",
        "prediction=model2.predict(np.array([pretraitement(\"I love this\")]))[0][0] # testons notre pipeline avec une phrase\r\n",
        "print(prediction)\r\n",
        "if (prediction<0.4):\r\n",
        "  print(\"This comment is negative.\")\r\n",
        "elif (prediction>0.6):\r\n",
        "  print(\"This comment is postive.\")\r\n",
        "else:\r\n",
        "  print(\"This comment is neutral.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4752633\n",
            "This comment is neutral.\n",
            "0.4752633\n",
            "This comment is neutral.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EMmArhF2qqa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}